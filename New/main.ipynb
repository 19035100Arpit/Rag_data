{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4ac1f2",
   "metadata": {},
   "source": [
    "# Step 1: Prepare Unified Dataset from Swagger JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eb62ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Unified API dataset (with full schema documentation & dependencies) saved: 1024 APIs\n",
      "ðŸ“„ File: api_unified_data_full.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Optional, Tuple, Set\n",
    "\n",
    "INPUT_FILE = \"swagger.json\"\n",
    "OUTPUT_FILE = \"api_unified_data_full.json\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load OpenAPI (3.x) document\n",
    "# -----------------------------\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    doc: Dict[str, Any] = json.load(f)\n",
    "\n",
    "# Basic handles\n",
    "components: Dict[str, Any] = doc.get(\"components\", {}) or {}\n",
    "schemas: Dict[str, Any]    = components.get(\"schemas\", {}) or {}\n",
    "parameters_comp: Dict[str, Any] = components.get(\"parameters\", {}) or {}\n",
    "responses_comp: Dict[str, Any]  = components.get(\"responses\", {}) or {}\n",
    "paths: Dict[str, Any]      = doc.get(\"paths\", {}) or {}\n",
    "\n",
    "# -----------------------------\n",
    "# JSON Pointer ($ref) resolver\n",
    "# -----------------------------\n",
    "def resolve_pointer(doc_obj: Dict[str, Any], ref: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Resolve JSON pointer of the form '#/a/b/c' anywhere in the OpenAPI doc.\n",
    "    Returns a dict or None if not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(ref, str) or not ref.startswith(\"#/\"):\n",
    "        return None\n",
    "    parts = ref.lstrip(\"#/\").split(\"/\")\n",
    "    cur: Any = doc_obj\n",
    "    for p in parts:\n",
    "        if isinstance(cur, dict) and p in cur:\n",
    "            cur = cur[p]\n",
    "        else:\n",
    "            return None\n",
    "    if isinstance(cur, dict):\n",
    "        return cur\n",
    "    return None\n",
    "\n",
    "def deep_copy(obj: Any) -> Any:\n",
    "    return json.loads(json.dumps(obj))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Schema flattening (handles allOf/oneOf)\n",
    "# ---------------------------------------\n",
    "def merge_dict(dst: Dict[str, Any], src: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Shallow merge with priority to existing dst keys; merges nested 'properties' and 'required'\n",
    "    safely for OpenAPI objects.\n",
    "    \"\"\"\n",
    "    out = deep_copy(dst)\n",
    "    for k, v in (src or {}).items():\n",
    "        if k == \"properties\" and isinstance(v, dict):\n",
    "            out.setdefault(\"properties\", {})\n",
    "            out[\"properties\"].update(v)\n",
    "        elif k == \"required\" and isinstance(v, list):\n",
    "            out.setdefault(\"required\", [])\n",
    "            # keep order, avoid duplicates\n",
    "            for item in v:\n",
    "                if item not in out[\"required\"]:\n",
    "                    out[\"required\"].append(item)\n",
    "        else:\n",
    "            # don't overwrite existing non-empty values unless empty\n",
    "            if k not in out or out[k] in (None, \"\", [], {}):\n",
    "                out[k] = v\n",
    "    return out\n",
    "\n",
    "def flatten_schema(schema: Dict[str, Any], visited: Set[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Recursively resolve $ref and flatten allOf/oneOf/anyOf blocks.\n",
    "    Returns a single merged schema dict (best-effort).\n",
    "    \"\"\"\n",
    "    if not isinstance(schema, dict):\n",
    "        return {}\n",
    "\n",
    "    # Resolve $ref first\n",
    "    if \"$ref\" in schema:\n",
    "        target = resolve_pointer(doc, schema[\"$ref\"])\n",
    "        if target is None:\n",
    "            return {\"$ref_unresolved\": schema[\"$ref\"]}\n",
    "        # guard against cycles\n",
    "        ref_key = schema[\"$ref\"]\n",
    "        if ref_key in visited:\n",
    "            return {\"$ref_cycle\": ref_key}\n",
    "        visited.add(ref_key)\n",
    "        resolved = flatten_schema(target, visited)\n",
    "        # allow local overrides (e.g., description, nullable on the $ref)\n",
    "        return merge_dict(resolved, {k: v for k, v in schema.items() if k != \"$ref\"})\n",
    "\n",
    "    out = deep_copy(schema)\n",
    "\n",
    "    # Handle composition\n",
    "    for key in (\"allOf\", \"oneOf\", \"anyOf\"):\n",
    "        if key in out and isinstance(out[key], list):\n",
    "            merged = {}\n",
    "            for part in out[key]:\n",
    "                merged = merge_dict(merged, flatten_schema(part, visited))\n",
    "            # prefer merged over list\n",
    "            out.pop(key, None)\n",
    "            out = merge_dict(out, merged)\n",
    "\n",
    "    # Flatten items for arrays\n",
    "    if out.get(\"type\") == \"array\" and \"items\" in out:\n",
    "        out[\"items\"] = flatten_schema(out[\"items\"], visited)\n",
    "\n",
    "    # Flatten properties for objects\n",
    "    if out.get(\"type\") == \"object\" and \"properties\" in out:\n",
    "        props = out.get(\"properties\", {})\n",
    "        flat_props = {}\n",
    "        for pname, pschema in props.items():\n",
    "            flat_props[pname] = flatten_schema(pschema, visited)\n",
    "        out[\"properties\"] = flat_props\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Pretty print of schema into a multiline documentation\n",
    "# -------------------------------------------------------\n",
    "def schema_to_text(schema: Dict[str, Any], depth: int = 0) -> str:\n",
    "    \"\"\"\n",
    "    Turn a schema dict into a human-readable multiline string.\n",
    "    \"\"\"\n",
    "    indent = \"  \" * depth\n",
    "    if not schema:\n",
    "        return f\"{indent}- (empty)\\n\"\n",
    "\n",
    "    out = \"\"\n",
    "    if \"$ref_unresolved\" in schema:\n",
    "        out += f\"{indent}- $ref unresolved: {schema['$ref_unresolved']}\\n\"\n",
    "        return out\n",
    "    if \"$ref_cycle\" in schema:\n",
    "        out += f\"{indent}- $ref cycle detected: {schema['$ref_cycle']}\\n\"\n",
    "        return out\n",
    "\n",
    "    stype = schema.get(\"type\", \"object\")\n",
    "    fmt   = schema.get(\"format\")\n",
    "    desc  = schema.get(\"description\")\n",
    "    enum  = schema.get(\"enum\")\n",
    "    nullable = schema.get(\"nullable\", False)\n",
    "\n",
    "    out += f\"{indent}- Type: {stype}{f' ({fmt})' if fmt else ''}{' (nullable)' if nullable else ''}\\n\"\n",
    "    if desc:\n",
    "        out += f\"{indent}  Description: {desc}\\n\"\n",
    "    if enum and isinstance(enum, list) and len(enum) > 0:\n",
    "        out += f\"{indent}  Enum: {', '.join(map(str, enum))}\\n\"\n",
    "\n",
    "    # Object properties\n",
    "    if stype == \"object\":\n",
    "        required = schema.get(\"required\", []) or []\n",
    "        props = schema.get(\"properties\", {}) or {}\n",
    "        if props:\n",
    "            for pname, pschema in props.items():\n",
    "                req_flag = \" (required)\" if pname in required else \"\"\n",
    "                out += f\"{indent}  Field: {pname}{req_flag}\\n\"\n",
    "                out += schema_to_text(pschema, depth + 2)\n",
    "\n",
    "    # Arrays\n",
    "    if stype == \"array\":\n",
    "        items = schema.get(\"items\", {})\n",
    "        out += f\"{indent}  Items:\\n\"\n",
    "        out += schema_to_text(items, depth + 2)\n",
    "\n",
    "    # Example/Default\n",
    "    if \"example\" in schema:\n",
    "        out += f\"{indent}  Example: {json.dumps(schema['example'], ensure_ascii=False)}\\n\"\n",
    "    if \"default\" in schema:\n",
    "        out += f\"{indent}  Default: {schema['default']}\\n\"\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Helper: gather schema names used by an entity\n",
    "# ---------------------------------------------\n",
    "def collect_ref_names(schema: Dict[str, Any]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Collect component schema names referenced inside a schema dict\n",
    "    (via $ref to #/components/schemas/NAME). Useful for dependency graph.\n",
    "    \"\"\"\n",
    "    found: Set[str] = set()\n",
    "\n",
    "    def walk(node: Any):\n",
    "        if isinstance(node, dict):\n",
    "            if \"$ref\" in node and isinstance(node[\"$ref\"], str):\n",
    "                ref = node[\"$ref\"]\n",
    "                if ref.startswith(\"#/components/schemas/\"):\n",
    "                    found.add(ref.split(\"/\")[-1])\n",
    "            # recurse\n",
    "            for v in node.values():\n",
    "                walk(v)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                walk(item)\n",
    "\n",
    "    walk(schema)\n",
    "    return found\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Extract parameter (inline or $ref) + schema\n",
    "# ---------------------------------------------\n",
    "def extract_parameter(param_obj: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns (normalized_param, flattened_schema)\n",
    "    \"\"\"\n",
    "    # Resolve $ref parameter\n",
    "    if \"$ref\" in param_obj:\n",
    "        p_res = resolve_pointer(doc, param_obj[\"$ref\"]) or {}\n",
    "        base = deep_copy(p_res)\n",
    "        # allow local overrides\n",
    "        for k, v in param_obj.items():\n",
    "            if k != \"$ref\":\n",
    "                base[k] = v\n",
    "        param_obj = base\n",
    "\n",
    "    schema = param_obj.get(\"schema\", {}) or {}\n",
    "    flat = flatten_schema(schema, visited=set())\n",
    "    normalized = {\n",
    "        \"name\": param_obj.get(\"name\"),\n",
    "        \"in\": param_obj.get(\"in\"),\n",
    "        \"required\": bool(param_obj.get(\"required\", False)),\n",
    "        \"description\": param_obj.get(\"description\", \"\"),\n",
    "    }\n",
    "    return normalized, flat\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Extract requestBody across content types\n",
    "# ---------------------------------------------\n",
    "def extract_request_body(operation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    rb = operation.get(\"requestBody\", {})\n",
    "    if not isinstance(rb, dict) or not rb:\n",
    "        return {\"schema_text\": \"\", \"schema_flat\": {}, \"content_types\": []}\n",
    "\n",
    "    content = rb.get(\"content\", {}) or {}\n",
    "    content_types = list(content.keys())\n",
    "\n",
    "    # Prefer application/json when available\n",
    "    chosen_ct = \"application/json\" if \"application/json\" in content else (content_types[0] if content_types else None)\n",
    "\n",
    "    schema_flat = {}\n",
    "    schema_text = \"\"\n",
    "    if chosen_ct:\n",
    "        schema_obj = content.get(chosen_ct, {}).get(\"schema\", {}) or {}\n",
    "        flat = flatten_schema(schema_obj, visited=set())\n",
    "        schema_flat = flat\n",
    "        schema_text = f\"{chosen_ct}:\\n\" + schema_to_text(flat, depth=1)\n",
    "    else:\n",
    "        schema_text = \"\"\n",
    "\n",
    "    return {\n",
    "        \"schema_text\": schema_text.strip(),\n",
    "        \"schema_flat\": schema_flat,\n",
    "        \"content_types\": content_types,\n",
    "        \"required\": bool(rb.get(\"required\", False)),\n",
    "        \"description\": rb.get(\"description\", \"\")\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Extract responses (iterate all status codes)\n",
    "# ---------------------------------------------\n",
    "def extract_responses(operation: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {}\n",
    "    responses = operation.get(\"responses\", {}) or {}\n",
    "    for status, resp_obj in responses.items():\n",
    "        # can be $ref response\n",
    "        if \"$ref\" in resp_obj:\n",
    "            resolved = resolve_pointer(doc, resp_obj[\"$ref\"]) or {}\n",
    "            resp_obj = merge_dict(resolved, {k: v for k, v in resp_obj.items() if k != \"$ref\"})\n",
    "\n",
    "        desc = resp_obj.get(\"description\", \"\")\n",
    "        content = resp_obj.get(\"content\", {}) or {}\n",
    "        cts = list(content.keys())\n",
    "        # Prefer application/json\n",
    "        chosen_ct = \"application/json\" if \"application/json\" in content else (cts[0] if cts else None)\n",
    "\n",
    "        flat = {}\n",
    "        text = \"\"\n",
    "        if chosen_ct:\n",
    "            schema_obj = content.get(chosen_ct, {}).get(\"schema\", {}) or {}\n",
    "            flat = flatten_schema(schema_obj, visited=set())\n",
    "            text = f\"{chosen_ct}:\\n\" + schema_to_text(flat, depth=1)\n",
    "\n",
    "        out[status] = {\n",
    "            \"description\": desc,\n",
    "            \"content_types\": cts,\n",
    "            \"schema_text\": text.strip(),\n",
    "            \"schema_flat\": flat\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Path-level parameters (apply to all operations)\n",
    "# ---------------------------------------------------\n",
    "def gather_path_parameters(path_item: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    params = path_item.get(\"parameters\", []) or []\n",
    "    normalized = []\n",
    "    for p in params:\n",
    "        p_norm, p_flat = extract_parameter(p)\n",
    "        p_norm[\"schema_text\"] = schema_to_text(p_flat, depth=1).strip()\n",
    "        p_norm[\"schema_flat\"] = p_flat\n",
    "        normalized.append(p_norm)\n",
    "    return normalized\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Build unified dataset\n",
    "# ---------------------------------------------------\n",
    "api_unified_data: List[Dict[str, Any]] = []\n",
    "\n",
    "for path, path_item in paths.items():\n",
    "    if not isinstance(path_item, dict):\n",
    "        continue\n",
    "\n",
    "    # Path-scoped params\n",
    "    path_params = gather_path_parameters(path_item)\n",
    "\n",
    "    for method, operation in path_item.items():\n",
    "        if method.lower() not in (\"get\", \"post\", \"put\", \"patch\", \"delete\", \"options\", \"head\"):\n",
    "            continue\n",
    "        if not isinstance(operation, dict):\n",
    "            continue\n",
    "        operation_id = operation.get(\"operationId\", \"\")\n",
    "        summary = (operation.get(\"summary\") or \"\").strip()\n",
    "        description = (operation.get(\"description\") or \"\").strip()\n",
    "        tags = operation.get(\"tags\", []) or []\n",
    "\n",
    "        # Parameters (operation + path-level)\n",
    "        op_params = operation.get(\"parameters\", []) or []\n",
    "        param_details: List[Dict[str, Any]] = []\n",
    "\n",
    "        # Include path-level params first (as inherited)\n",
    "        for pp in path_params:\n",
    "            param_details.append(pp)\n",
    "\n",
    "        # Then operation-level params\n",
    "        for p in op_params:\n",
    "            p_norm, p_flat = extract_parameter(p)\n",
    "            p_norm[\"schema_text\"] = schema_to_text(p_flat, depth=1).strip()\n",
    "            p_norm[\"schema_flat\"] = p_flat\n",
    "            param_details.append(p_norm)\n",
    "\n",
    "        # Request body\n",
    "        request_body_info = extract_request_body(operation)\n",
    "        req_schema_text = request_body_info[\"schema_text\"]\n",
    "        req_schema_flat  = request_body_info[\"schema_flat\"]\n",
    "\n",
    "        # Responses\n",
    "        responses_info = extract_responses(operation)\n",
    "        # Build a consolidated text block for responses\n",
    "        resp_schema_lines = []\n",
    "        for status_code, info in responses_info.items():\n",
    "            resp_schema_lines.append(f\"Status {status_code}:\\n{info['schema_text']}\" if info[\"schema_text\"] else f\"Status {status_code}:\")\n",
    "        resp_schema_text = \"\\n\".join([l for l in resp_schema_lines if l]).strip()\n",
    "\n",
    "        entry = {\n",
    "            \"path\": path,\n",
    "            \"method\": method.upper(),\n",
    "            \"operation_id\": operation_id,\n",
    "            \"tags\": tags,\n",
    "            \"summary\": summary,\n",
    "            \"description\": description,\n",
    "            \"parameters\": param_details,\n",
    "            \"request_body\": {\n",
    "                \"description\": request_body_info.get(\"description\", \"\"),\n",
    "                \"required\": request_body_info.get(\"required\", False),\n",
    "                \"content_types\": request_body_info.get(\"content_types\", []),\n",
    "                \"schema_text\": req_schema_text,\n",
    "                \"schema_flat\": req_schema_flat,\n",
    "            },\n",
    "            \"responses\": responses_info,  # keep structured per status\n",
    "            \"responses_schema_text\": resp_schema_text,\n",
    "        }\n",
    "        api_unified_data.append(entry)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Dependency map (schema-name level intersection)\n",
    "# ---------------------------------------------------\n",
    "# For each API: collect response schema names; for each target: collect request schema names (params+body)\n",
    "resp_schema_names_by_idx: List[Set[str]] = []\n",
    "req_schema_names_by_idx: List[Set[str]]  = []\n",
    "\n",
    "for entry in api_unified_data:\n",
    "    # Responses set\n",
    "    rset: Set[str] = set()\n",
    "    for _status, info in (entry.get(\"responses\") or {}).items():\n",
    "        flat = info.get(\"schema_flat\") or {}\n",
    "        rset |= collect_ref_names(flat)\n",
    "    resp_schema_names_by_idx.append(rset)\n",
    "\n",
    "    # Requests set (body + params)\n",
    "    qset: Set[str] = set()\n",
    "    # body\n",
    "    qset |= collect_ref_names(entry.get(\"request_body\", {}).get(\"schema_flat\") or {})\n",
    "    # params\n",
    "    for p in entry.get(\"parameters\", []) or []:\n",
    "        qset |= collect_ref_names(p.get(\"schema_flat\") or {})\n",
    "    req_schema_names_by_idx.append(qset)\n",
    "\n",
    "dependency_map: Dict[int, List[int]] = defaultdict(list)\n",
    "for i, rnames in enumerate(resp_schema_names_by_idx):\n",
    "    if not rnames:\n",
    "        continue\n",
    "    for j, qnames in enumerate(req_schema_names_by_idx):\n",
    "        if i == j:\n",
    "            continue\n",
    "        # If any response schema name of i appears in request schema names of j -> i feeds j\n",
    "        if rnames & qnames:\n",
    "            dependency_map[i].append(j)\n",
    "\n",
    "# Attach \"calls_next\" by path\n",
    "for i, entry in enumerate(api_unified_data):\n",
    "    next_indices = dependency_map.get(i, [])\n",
    "    next_paths = []\n",
    "    for j in next_indices:\n",
    "        target_path = api_unified_data[j][\"path\"]\n",
    "        if target_path not in next_paths:\n",
    "            next_paths.append(target_path)\n",
    "    entry[\"calls_next\"] = next_paths\n",
    "\n",
    "# -----------------------------\n",
    "# Save output\n",
    "# -----------------------------\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(api_unified_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Unified API dataset (with full schema documentation & dependencies) saved: {len(api_unified_data)} APIs\")\n",
    "print(f\"ðŸ“„ File: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3abfd6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import json\n",
    "# import uuid\n",
    "# from datetime import datetime\n",
    " \n",
    "# # === CONFIG ===\n",
    "# SWAGGER_FILE = \"swagger.json\"          # your Swagger/OpenAPI file\n",
    "# OUTPUT_FILE = \"api_dataset_cleaned.json\"\n",
    " \n",
    "# # === LOAD SWAGGER FILE ===\n",
    "# with open(SWAGGER_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "#     swagger_data = json.load(f)\n",
    " \n",
    "# print(f\"Loaded Swagger file successfully\")\n",
    " \n",
    "# # === HELPER FUNCTION TO PARSE PARAMETERS ===\n",
    "# def extract_parameters(api_data):\n",
    "#     parameters = []\n",
    "#     if \"parameters\" in api_data:\n",
    "#         for param in api_data[\"parameters\"]:\n",
    "#             parameters.append({\n",
    "#                 \"name\": param.get(\"name\", \"\"),\n",
    "#                 \"in\": param.get(\"in\", \"body\"),\n",
    "#                 \"type\": param.get(\"schema\", {}).get(\"type\", \"string\"),\n",
    "#                 \"required\": param.get(\"required\", False),\n",
    "#                 \"description\": param.get(\"description\", \"\")\n",
    "#             })\n",
    "#     # Handle requestBody schema (OpenAPI 3.x)\n",
    "#     if \"requestBody\" in api_data:\n",
    "#         try:\n",
    "#             props = api_data[\"requestBody\"][\"content\"][\"application/json\"][\"schema\"][\"properties\"]\n",
    "#             for key, val in props.items():\n",
    "#                 parameters.append({\n",
    "#                     \"name\": key,\n",
    "#                     \"in\": \"body\",\n",
    "#                     \"type\": val.get(\"type\", \"string\"),\n",
    "#                     \"required\": True,\n",
    "#                     \"description\": val.get(\"description\", \"\")\n",
    "#                 })\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#     return parameters\n",
    " \n",
    "# # === HELPER FUNCTION TO PARSE RESPONSES ===\n",
    "# def extract_responses(api_data):\n",
    "#     responses = {}\n",
    "#     for code, detail in api_data.get(\"responses\", {}).items():\n",
    "#         responses[code] = {\"description\": detail.get(\"description\", \"\")}\n",
    "#     return responses\n",
    " \n",
    "# # === MAIN EXTRACTION ===\n",
    "# api_entries = []\n",
    " \n",
    "# for path, methods in swagger_data.get(\"paths\", {}).items():\n",
    "#     for method, api_data in methods.items():\n",
    "#         api_id = api_data.get(\"operationId\") or f\"{method}_{path.strip('/').replace('/', '_')}\"\n",
    "#         summary = api_data.get(\"summary\", \"\")\n",
    "#         description = api_data.get(\"description\", \"\")\n",
    " \n",
    "#         entry = {\n",
    "#             \"api_id\": api_id,\n",
    "#             \"path\": path,\n",
    "#             \"method\": method.upper(),\n",
    "#             \"summary\": summary,\n",
    "#             \"description\": description,\n",
    "#             \"parameters\": extract_parameters(api_data),\n",
    "#             \"responses\": extract_responses(api_data),\n",
    "#             \"category\": (path.split(\"/\")[1] if len(path.split(\"/\")) > 1 else \"general\"),\n",
    "#             \"tags\": api_data.get(\"tags\", []),\n",
    "#             \"example_request\": {},\n",
    "#             \"example_response\": {},\n",
    "#             \"source_file\": SWAGGER_FILE,\n",
    "#             \"last_updated\": datetime.now().strftime(\"%Y-%m-%d\")\n",
    "          \n",
    "#         }\n",
    " \n",
    "#         api_entries.append(entry)\n",
    " \n",
    "# print(f\" Extracted {len(api_entries)} endpoints from Swagger\")\n",
    " \n",
    "# # === SAVE TO CLEANED DATASET ===\n",
    "# with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(api_entries, f, indent=2, ensure_ascii=False)\n",
    " \n",
    "# print(f\"\\n Final dataset saved to: {OUTPUT_FILE}\")\n",
    "# print(f\"Total APIs processed: {len(api_entries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e7927",
   "metadata": {},
   "source": [
    "# Step 2: Generate Embeddings for APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55c35415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded dataset with 1024 API entries\n",
      " Loading model: C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\models\\all-MiniLM-L6-v2\n",
      " Creating embeddings for 1024 entries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202661be3bb542939dd89b1fb547ae72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FAISS index saved to: updated_api_index.faiss\n",
      " Saved API metadata to updated_api_mapping.json\n",
      "\n",
      " Embedding generation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    " \n",
    "# === CONFIG ===\n",
    "DATA_FILE = \"api_unified_data_full.json\"\n",
    "INDEX_FILE = \"updated_api_index.faiss\"\n",
    "EMBED_MODEL = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\models\\all-MiniLM-L6-v2\"\n",
    " \n",
    "# === LOAD DATA ===\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    api_data = json.load(f)\n",
    " \n",
    "print(f\" Loaded dataset with {len(api_data)} API entries\")\n",
    " \n",
    "# === LOAD EMBEDDING MODEL ===\n",
    "print(f\" Loading model: {EMBED_MODEL}\")\n",
    "model = SentenceTransformer(EMBED_MODEL)\n",
    " \n",
    "# === PREPARE TEXT CORPUS FOR EACH API ===\n",
    "corpus = []\n",
    "for api in api_data:\n",
    "    text = f\"{api['path']} | {api['method']} | {api['summary']} | {api['description']} | {' '.join(api.get('tags', []))}\"\n",
    "    corpus.append(text)\n",
    " \n",
    "print(f\" Creating embeddings for {len(corpus)} entries...\")\n",
    " \n",
    "# === GENERATE EMBEDDINGS ===\n",
    "embeddings = model.encode(corpus, show_progress_bar=True)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    " \n",
    "# === CREATE & SAVE FAISS INDEX ===\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    " \n",
    "faiss.write_index(index, INDEX_FILE)\n",
    "print(f\" FAISS index saved to: {INDEX_FILE}\")\n",
    " \n",
    "# === SAVE MAPPING (API Metadata) ===\n",
    "with open(\"updated_api_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(api_data, f, indent=2, ensure_ascii=False)\n",
    " \n",
    "print(\" Saved API metadata to updated_api_mapping.json\")\n",
    " \n",
    "print(\"\\n Embedding generation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a64ee",
   "metadata": {},
   "source": [
    "# Step 3.1 : Load Models & Metadata ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "560abb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading API metadata...\n",
      "Loaded 1024 API entries\n",
      "Loading embedding model...\n",
      "Loading FAISS index...\n",
      "FAISS index loaded with 1024 vectors\n",
      "Building BM25 index...\n",
      "BM25 index built successfully \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    " \n",
    "# === CONFIG ===\n",
    "API_MAPPING_FILE = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\updated_api_mapping.json\"\n",
    "FAISS_INDEX_FILE = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\updated_api_index.faiss\"\n",
    "EMBED_MODEL = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\models\\all-MiniLM-L6-v2\"\n",
    " \n",
    "# === Load Metadata ===\n",
    "print(\"Loading API metadata...\")\n",
    "with open(API_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    api_metadata = json.load(f)\n",
    "print(f\"Loaded {len(api_metadata)} API entries\")\n",
    " \n",
    "# === Load Embedding Model ===\n",
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL)\n",
    " \n",
    "# === Load FAISS Index ===\n",
    "print(\"Loading FAISS index...\")\n",
    "index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "print(f\"FAISS index loaded with {index.ntotal} vectors\")\n",
    " \n",
    "# === Build BM25 (optional) ===\n",
    "print(\"Building BM25 index...\")\n",
    "bm25_corpus = []\n",
    "for api in api_metadata:\n",
    "    txt = f\"{api.get('path','')} {api.get('method','')} {api.get('summary','')} {api.get('description','')}\"\n",
    "    bm25_corpus.append(txt.split())\n",
    " \n",
    "bm25 = BM25Okapi(bm25_corpus)\n",
    "print(\"BM25 index built successfully \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618cde4c",
   "metadata": {},
   "source": [
    "# Steps 3.2 : Hybrid Retrieval ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d39d9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 candidates for: Which api responsible for get the booking dtails?\n",
      "/api/nsk/v1/vouchers/configuration  [GET]  â†’ 3.263\n",
      "/api/nsk/v1/booking/payments/{paymentMethod}/dcc  [POST]  â†’ 2.357\n",
      "/api/nsk/v6/booking/payments/dcc/{dccKey}  [POST]  â†’ 2.023\n",
      "/api/nsk/v1/vouchers/configuration/{configurationCode}  [GET]  â†’ 1.954\n",
      "/api/nsk/v2/booking/payments/available  [GET]  â†’ 1.735\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    " \n",
    "def embed_query(query: str):\n",
    "    \"\"\"Encode query into dense vector\"\"\"\n",
    "    vec = embed_model.encode([query], convert_to_numpy=True)\n",
    "    return vec.astype(\"float32\")\n",
    " \n",
    "def dense_search(query_vec, top_k=20):\n",
    "    \"\"\"FAISS dense vector search\"\"\"\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    results = []\n",
    "    for idx, dist in zip(I[0], D[0]):\n",
    "        sim = -float(dist)  # higher = better\n",
    "        results.append((int(idx), sim))\n",
    "    return results\n",
    " \n",
    "def bm25_search(query, top_k=20):\n",
    "    \"\"\"BM25 sparse text search\"\"\"\n",
    "    tokenized = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized)\n",
    "    ranked = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(int(i), float(scores[i])) for i in ranked]\n",
    " \n",
    "def hybrid_search(query, top_k=20, bm25_weight=0.3):\n",
    "    \"\"\"Combine dense + BM25 scores\"\"\"\n",
    "    qvec = embed_query(query)\n",
    "    dense = dense_search(qvec, top_k)\n",
    "    dense_scores = {i: s for i, s in dense}\n",
    " \n",
    "    bm25_results = bm25_search(query, top_k)\n",
    "    bm25_scores = {i: s for i, s in bm25_results}\n",
    " \n",
    "    all_ids = set(list(dense_scores.keys()) + list(bm25_scores.keys()))\n",
    "    hybrid = []\n",
    "    for idx in all_ids:\n",
    "        score = dense_scores.get(idx, 0) * (1 - bm25_weight) + bm25_scores.get(idx, 0) * bm25_weight\n",
    "        hybrid.append((idx, score))\n",
    " \n",
    "    hybrid = sorted(hybrid, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    print(f\"Retrieved {len(hybrid)} candidates for: {query}\")\n",
    "    return [{\"id\": i, \"score\": s, \"meta\": api_metadata[i]} for i, s in hybrid]\n",
    " # --- Example ---\n",
    "query = \"Which api responsible for get the booking dtails?\"\n",
    "results = hybrid_search(query, top_k=10)\n",
    "for r in results[:5]:\n",
    "    \n",
    "    print(f\"{r['meta']['path']}  [{r['meta']['method']}]  â†’ {r['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eabad0b",
   "metadata": {},
   "source": [
    "# Step 3.3 : Rerank Top Results ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02f0ae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cross-encoder reranker...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import CrossEncoder  # pip install cross-encoder\n",
    " \n",
    "# === Load Reranker ===\n",
    "print(\"Loading cross-encoder reranker...\")\n",
    "reranker = CrossEncoder(r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\Reranking-Model\\local-ms-marco\")\n",
    " \n",
    "def rerank_with_cross_encoder(query, candidates, top_k=5):\n",
    "    pairs = []\n",
    "    for c in candidates:\n",
    "        text = f\"{c['meta'].get('path','')} {c['meta'].get('summary','')} {c['meta'].get('description','')}\"\n",
    "        pairs.append((query, text))\n",
    " \n",
    "    scores = reranker.predict(pairs)\n",
    "    for i, s in enumerate(scores):\n",
    "        candidates[i]['rerank_score'] = float(s)\n",
    " \n",
    "    ranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)\n",
    "    return ranked[:top_k]\n",
    " \n",
    "# # --- Example ---\n",
    "# query = \"Which api responsible for get the booking dtails?\"\n",
    "# top_candidates = rerank_with_cross_encoder(query, results, top_k=5)\n",
    "# for r in top_candidates:\n",
    "#     print(f\"{r['meta']['path']} â†’ {r['rerank_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d89ca9",
   "metadata": {},
   "source": [
    "# Step 3.4 : Generate Step-by-Step Plan ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47c49c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    " \n",
    "def generate_plan(query, top_candidates):\n",
    "    \"\"\"Create a step-by-step plan template\"\"\"\n",
    "    lines = [f\"User Query: {query}\", \"\\nSuggested API Workflow:\\n\"]\n",
    "    for i, c in enumerate(top_candidates, 1):\n",
    "        meta = c['meta']\n",
    "        lines.append(f\"Step {i}: {meta.get('summary','')}\")\n",
    "        lines.append(f\"  Endpoint: {meta.get('path')}  |  Method: {meta.get('method')}\")\n",
    "        params = meta.get('parameters', [])\n",
    "        if params:\n",
    "            param_names = [p.get('name') for p in params]\n",
    "            lines.append(f\"  Parameters: {', '.join(param_names)}\")\n",
    "        lines.append(\"\")  # blank line\n",
    "    return \"\\n\".join(lines)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92456bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Which api responsible for check in flow\n",
      "\n",
      "Suggested API Workflow:\n",
      "\n",
      "Step 1: Creates a passive segment and creates a booking if it doesn't already exist.\n",
      "  Endpoint: /api/nsk/v1/trip/passiveSegments  |  Method: POST\n",
      "\n",
      "Step 2: Retrieves the booking payment methods available for the booking in state.\n",
      "  Endpoint: /api/nsk/v2/booking/payments/available  |  Method: GET\n",
      "  Parameters: currencyCode\n",
      "\n",
      "Step 3: Gets credit available by reference number and type.\n",
      "  Endpoint: /api/nsk/v2/booking/payments/credit  |  Method: GET\n",
      "  Parameters: ReferenceNumber, CurrencyCode, Type\n",
      "\n",
      "Step 4: Retrieves the booking payment methods available for a refund on the booking in state.\n",
      "  Endpoint: /api/nsk/v1/booking/payments/refunds  |  Method: GET\n",
      "\n",
      "Step 5: Gets the list of seat maps for all the journeys for the booking in state.\n",
      "  Endpoint: /api/nsk/v3/booking/seatmaps  |  Method: GET\n",
      "  Parameters: FeePricingMode, CollectedCurrencyCode, IncludePropertyLookup, CultureCode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Example ---\n",
    "query = \"Which api responsible for check in flow\"\n",
    "plan = generate_plan(query, top_candidates)\n",
    "print(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d4dcf",
   "metadata": {},
   "source": [
    "# final Step  Agentic API Assistant ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ead65a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Public function: Answer a query ----------\n",
    "def answer_query(query: str, top_k=5, bm25_weight=0.35, use_reranker=True, base_url=None, verbose=True):\n",
    "    \"\"\"\n",
    "    End-to-end:\n",
    "      - Hybrid retrieve (FAISS + BM25)\n",
    "      - Optional rerank (cross-encoder if available) else RRF fallback\n",
    "      - Produce deterministic plan with sample calls\n",
    "\n",
    "    Returns a dict:\n",
    "      {\n",
    "        \"query\": ...,\n",
    "        \"candidates\": [ {path, method, score, rerank_score}, ... ],\n",
    "        \"plan_text\": \"...\",\n",
    "        \"raw\": { \"hybrid\": [...], \"reranked\": [...] }\n",
    "      }\n",
    "    \"\"\"\n",
    "    # Step 1: Hybrid retrieve\n",
    "    hybrid = hybrid_retrieve(query, top_k=max(top_k*2, 10), bm25_weight=bm25_weight)\n",
    "\n",
    "    # Step 2: Rerank\n",
    "    if use_reranker and reranker is not None:\n",
    "        reranked = rerank_candidates(query, hybrid, top_k=top_k)\n",
    "    else:\n",
    "        # RRF fallback (non-LLM; robust)\n",
    "        qvec = embed_query(normalize_query(query))\n",
    "        dense = dense_search(qvec, top_k=max(top_k*2, 10))\n",
    "        bm25_res = bm25_search(normalize_query(query), top_k=max(top_k*2, 10))\n",
    "        reranked = rrf_fusion(dense, bm25_res, top_k=top_k)\n",
    "        for r in reranked:\n",
    "            r[\"rerank_score\"] = None\n",
    "\n",
    "    # Step 3: Build plan (deterministic, includes examples)\n",
    "    plan_text = template_plan(query, reranked, base_url=base_url)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nTop candidates:\")\n",
    "        for i, r in enumerate(reranked, 1):\n",
    "            m = r[\"meta\"]\n",
    "            s = r.get(\"rerank_score\", None)\n",
    "            print(f\"{i}. {m.get('path')}  [{m.get('method')}]  \"\n",
    "                  f\"hybrid_score={r.get('score'):.3f}\"\n",
    "                  + (f\"  rerank_score={s:.3f}\" if s is not None else \"\"))\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(plan_text)\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    candidates_view = [{\n",
    "        \"path\": r[\"meta\"].get(\"path\"),\n",
    "        \"method\": r[\"meta\"].get(\"method\"),\n",
    "        \"score\": r.get(\"score\"),\n",
    "        \"rerank_score\": r.get(\"rerank_score\")\n",
    "    } for r in reranked]\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"candidates\": candidates_view,\n",
    "        \"plan_text\": plan_text,\n",
    "        \"raw\": {\n",
    "            \"hybrid\": hybrid,\n",
    "            \"reranked\": reranked\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9829e391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading API metadata...\n",
      "Loaded 1024 API entries\n",
      "Loading FAISS index...\n",
      "FAISS index loaded. vectors: 1024\n",
      "Loading embedding model: C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\models\\all-MiniLM-L6-v2 ...\n",
      "Embedding model loaded.\n",
      "Building BM25 index...\n",
      "BM25 ready.\n",
      "Reranker not available at 'CrossEncoder(\n",
      "  (model): BertForSequenceClassification(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 384)\n",
      "        (token_type_embeddings): Embedding(2, 384)\n",
      "        (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Linear(in_features=384, out_features=1, bias=True)\n",
      "  )\n",
      "  (activation_fn): Identity()\n",
      ")'. Using RRF fallback.\n",
      "\n",
      "Initialization complete âœ…  You can now call:\n",
      "  print(response)\n"
     ]
    }
   ],
   "source": [
    "# ====== Initialization Cell (Run ONCE) ======\n",
    "# Loads embedding model, FAISS index, metadata, BM25, (optional) reranker.\n",
    "# After this cell, call: answer_query(\"your question\") as many times as you want.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -------- CONFIG (update paths if needed) ----------\n",
    "API_MAPPING_FILE = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\api_unified_data_full.json\"    # output from Step 2\n",
    "FAISS_INDEX_FILE = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\updated_api_index.faiss\"     # output from Step 2\n",
    "EMBED_MODEL_NAME = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\models\\all-MiniLM-L6-v2\"\n",
    "\n",
    "# Optional local reranker (cross-encoder).\n",
    "# If this path is invalid or missing HF files, the code will safely fall back to RRF (non-LLM).\n",
    "RERANKER_MODEL = CrossEncoder(r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\Reranking-Model\\local-ms-marco\")\n",
    "# You can also use the hub id directly (requires internet):\n",
    "# RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "print(\"Loading API metadata...\")\n",
    "with open(API_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    api_metadata = json.load(f)\n",
    "print(f\"Loaded {len(api_metadata)} API entries\")\n",
    "\n",
    "print(\"Loading FAISS index...\")\n",
    "index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "print(\"FAISS index loaded. vectors:\", index.ntotal)\n",
    "\n",
    "print(f\"Loading embedding model: {EMBED_MODEL_NAME} ...\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# ---------- Text cleaning / tokenization ----------\n",
    "def clean_text_tokens(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', ' ', text).lower()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Add parameters & tags into corpus text to improve relevance\n",
    "def metadata_to_text(m):\n",
    "    params = m.get('parameters', []) or []\n",
    "    ptext = \" \".join([f\"{p.get('in','')}:{p.get('name','')}:{p.get('type','')}\" for p in params])\n",
    "    rtext = \" \".join([f\"{code}:{resp.get('description','')}\" for code, resp in (m.get('responses') or {}).items()])\n",
    "    return \" | \".join([\n",
    "        str(m.get('path','')),\n",
    "        str(m.get('method','')),\n",
    "        str(m.get('summary','')),\n",
    "        str(m.get('description','')),\n",
    "        \"tags:\" + \" \".join(m.get('tags',[])),\n",
    "        \"params:\" + ptext,\n",
    "        \"responses:\" + rtext\n",
    "    ])\n",
    "\n",
    "print(\"Building BM25 index...\")\n",
    "bm25_corpus = []\n",
    "for m in api_metadata:\n",
    "    txt = metadata_to_text(m)\n",
    "    bm25_corpus.append(clean_text_tokens(txt))\n",
    "bm25 = BM25Okapi(bm25_corpus)\n",
    "print(\"BM25 ready.\")\n",
    "\n",
    "# --------- (Optional) Reranker loading (safe) ----------\n",
    "reranker = None\n",
    "reranker_device = \"cpu\"\n",
    "\n",
    "def _has_hf_files(model_dir: str) -> bool:\n",
    "    \"\"\"Check if a local folder looks like a valid HF model (minimal check).\"\"\"\n",
    "    if not os.path.isdir(model_dir):\n",
    "        return False\n",
    "    needed = [\"config.json\"]  # minimal; you can add more like tokenizer_config.json, pytorch_model.bin\n",
    "    return all(os.path.isfile(os.path.join(model_dir, f)) for f in needed)\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    if isinstance(RERANKER_MODEL, str) and os.path.isdir(RERANKER_MODEL) and _has_hf_files(RERANKER_MODEL):\n",
    "        print(f\"Loading reranker from local path: {RERANKER_MODEL} ...\")\n",
    "        reranker = CrossEncoder(RERANKER_MODEL, device=reranker_device, local_files_only=True)\n",
    "        print(\"Reranker loaded (local).\")\n",
    "    elif isinstance(RERANKER_MODEL, str) and \"/\" in RERANKER_MODEL:\n",
    "        print(f\"Loading reranker from HF Hub: {RERANKER_MODEL} ...\")\n",
    "        reranker = CrossEncoder(RERANKER_MODEL, device=reranker_device)\n",
    "        print(\"Reranker loaded (hub).\")\n",
    "    else:\n",
    "        print(f\"Reranker not available at '{RERANKER_MODEL}'. Using RRF fallback.\")\n",
    "        reranker = None\n",
    "except Exception as e:\n",
    "    print(\"Failed to load reranker; continuing without it. Error:\", str(e))\n",
    "    reranker = None\n",
    "\n",
    "# --------- Utility functions ----------\n",
    "def normalize_query(q: str) -> str:\n",
    "    \"\"\"Lightweight typo fix & phrasing cleanup to improve retrieval without LLMs.\"\"\"\n",
    "    q_norm = (q or \"\").lower().strip()\n",
    "    fixes = {\n",
    "        \"dtails\": \"details\",\n",
    "        \"responsible for get\": \"get\",\n",
    "        \"api responsible for\": \"api for\",\n",
    "        \"booking dtail\": \"booking detail\",\n",
    "        \"bokking\": \"booking\",\n",
    "        \"resrvation\": \"reservation\",\n",
    "    }\n",
    "    for k, v in fixes.items():\n",
    "        q_norm = q_norm.replace(k, v)\n",
    "    q_norm = re.sub(r\"\\s+\", \" \", q_norm).strip()\n",
    "    return q_norm\n",
    "\n",
    "def embed_query(query: str):\n",
    "    vec = embed_model.encode([query], convert_to_numpy=True)\n",
    "    if vec.ndim == 1:\n",
    "        vec = vec.reshape(1, -1)\n",
    "    return vec.astype(\"float32\")\n",
    "\n",
    "def dense_search(query_vec, top_k=50):\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    results = []\n",
    "    for idx, dist in zip(I[0], D[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        # Convert L2 distance to a similarity-like score (bounded, higher=better)\n",
    "        sim = 1.0 / (1.0 + float(dist))\n",
    "        results.append((int(idx), sim))\n",
    "    return results\n",
    "\n",
    "def bm25_search(query, top_k=50):\n",
    "    tokens = clean_text_tokens(query)\n",
    "    scores = bm25.get_scores(tokens)\n",
    "    ranked = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(int(i), float(scores[i])) for i in ranked]\n",
    "\n",
    "def _minmax(arr):\n",
    "    \"\"\"Safe MinMax scaling for list/np array; returns zeros if degenerate.\"\"\"\n",
    "    arr = np.array(arr, dtype=float).reshape(-1,1)\n",
    "    if arr.size == 0:\n",
    "        return np.array([])\n",
    "    if np.allclose(arr.min(), arr.max()):\n",
    "        return np.zeros_like(arr).flatten()\n",
    "    return MinMaxScaler().fit_transform(arr).flatten()\n",
    "\n",
    "def hybrid_retrieve(query, top_k=20, bm25_weight=0.35):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval combining dense (FAISS) and BM25 with MinMax normalization.\n",
    "    Returns list of dicts: {'id', 'score', 'meta'}\n",
    "    \"\"\"\n",
    "    # Use normalized query for retrieval (keep original for display)\n",
    "    query_norm = normalize_query(query)\n",
    "\n",
    "    qvec = embed_query(query_norm)\n",
    "    dense = dense_search(qvec, top_k*2)\n",
    "    bm25_res = bm25_search(query_norm, top_k*2)\n",
    "\n",
    "    d_norm = _minmax([s for _, s in dense]) if len(dense) > 0 else np.array([])\n",
    "    b_norm = _minmax([s for _, s in bm25_res]) if len(bm25_res) > 0 else np.array([])\n",
    "\n",
    "    dense_dict = {dense[i][0]: float(d_norm[i]) for i in range(len(dense))} if len(dense) > 0 else {}\n",
    "    bm25_dict = {bm25_res[i][0]: float(b_norm[i]) for i in range(len(bm25_res))} if len(bm25_res) > 0 else {}\n",
    "\n",
    "    all_ids = set(list(dense_dict.keys()) + list(bm25_dict.keys()))\n",
    "    hybrid = []\n",
    "    for idx in all_ids:\n",
    "        d = dense_dict.get(idx, 0.0)\n",
    "        b = bm25_dict.get(idx, 0.0)\n",
    "        score = (1 - bm25_weight) * d + bm25_weight * b\n",
    "        hybrid.append((idx, float(score)))\n",
    "    hybrid = sorted(hybrid, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [{\"id\": idx, \"score\": s, \"meta\": api_metadata[idx]} for idx, s in hybrid]\n",
    "\n",
    "def rerank_candidates(query: str, candidates: list, top_k=5):\n",
    "    \"\"\"\n",
    "    Use cross-encoder to rerank candidate list if available.\n",
    "    Returns top_k list with 'rerank_score'.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "    if reranker is None:\n",
    "        ranked = sorted(candidates, key=lambda x: x[\"score\"], reverse=True)[:top_k]\n",
    "        for r in ranked:\n",
    "            r[\"rerank_score\"] = None\n",
    "        return ranked\n",
    "\n",
    "    pair_texts = []\n",
    "    for c in candidates:\n",
    "        m = c[\"meta\"]\n",
    "        text = metadata_to_text(m)\n",
    "        pair_texts.append((normalize_query(query), text))\n",
    "\n",
    "    scores = reranker.predict(pair_texts)\n",
    "    for i, s in enumerate(scores):\n",
    "        candidates[i][\"rerank_score\"] = float(s)\n",
    "    ranked = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)[:top_k]\n",
    "    return ranked\n",
    "\n",
    "# ---------- Parameter helpers & Output formatting ----------\n",
    "def _extract_params(m):\n",
    "    \"\"\"Normalized parameter records (optional; kept for future use).\"\"\"\n",
    "    params = m.get(\"parameters\", []) or []\n",
    "    out = []\n",
    "    for p in params:\n",
    "        out.append({\n",
    "            \"name\": p.get(\"name\", \"\"),\n",
    "            \"in\": p.get(\"in\", \"query\"),\n",
    "            \"type\": (p.get(\"type\") or (p.get(\"schema\", {}) or {}).get(\"type\") or \"string\"),\n",
    "            \"required\": bool(p.get(\"required\", False)),\n",
    "            \"description\": p.get(\"description\", \"\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def _param_names_only(m):\n",
    "    \"\"\"Return a comma-separated list of parameter NAMES (no type/in/required).\"\"\"\n",
    "    params = m.get(\"parameters\", []) or []\n",
    "    names = []\n",
    "    for p in params:\n",
    "        name = (p.get(\"name\") or \"\").strip()\n",
    "        if name and name != \"(body)\":\n",
    "            names.append(name)\n",
    "    # unique but keep order\n",
    "    seen = set()\n",
    "    uniq = [n for n in names if not (n in seen or seen.add(n))]\n",
    "    return \", \".join(uniq)\n",
    "\n",
    "def pretty_workflow_text(query: str, reranked: list) -> str:\n",
    "    \"\"\"\n",
    "    Prints exactly in the requested format:\n",
    "    User Query: ...\n",
    "    \n",
    "    Suggested API Workflow:\n",
    "    \n",
    "    Step i: <summary or path>.\n",
    "      Endpoint: <path>  |  Method: <METHOD>\n",
    "      Parameters: <CommaSeparatedParamNames>   # only if any\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    # Header\n",
    "    lines.append(f\"User Query: {query.strip()}\\n\")\n",
    "    lines.append(\"Suggested API Workflow:\\n\")\n",
    "\n",
    "    for i, c in enumerate(reranked, start=1):\n",
    "        m = c[\"meta\"]\n",
    "        summary = (m.get(\"summary\") or m.get(\"description\") or m.get(\"path\") or \"\").strip()\n",
    "        if summary and not summary.endswith(\".\"):\n",
    "            summary += \".\"\n",
    "        path = m.get(\"path\", \"\").strip()\n",
    "        method = (m.get(\"method\", \"GET\") or \"GET\").upper()\n",
    "        param_names = _param_names_only(m)\n",
    "\n",
    "        lines.append(f\"Step {i}: {summary}\")\n",
    "        lines.append(f\"  Endpoint: {path}  |  Method: {method}\")\n",
    "        if param_names:\n",
    "            lines.append(f\"  Parameters: {param_names}\")\n",
    "        lines.append(\"\")  # blank line after each step\n",
    "\n",
    "    # Remove trailing blank line\n",
    "    if lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- Reciprocal Rank Fusion (fallback if no reranker) ----------\n",
    "def rrf_fusion(dense_results, bm25_results, k=60, top_k=5):\n",
    "    \"\"\"\n",
    "    dense_results: list[(idx, score)] sorted desc by dense\n",
    "    bm25_results:  list[(idx, score)] sorted desc by bm25\n",
    "    Returns list of dicts like hybrid_retrieve but using RRF.\n",
    "    \"\"\"\n",
    "    def rank_map(res):\n",
    "        return {i: r for r, (i, _) in enumerate(res, start=1)}\n",
    "\n",
    "    d_sorted = sorted(dense_results, key=lambda x: x[1], reverse=True)\n",
    "    b_sorted = sorted(bm25_results, key=lambda x: x[1], reverse=True)\n",
    "    r_dense = rank_map(d_sorted)\n",
    "    r_bm25 = rank_map(b_sorted)\n",
    "\n",
    "    # Proper union of sets\n",
    "    all_ids = set(r_dense.keys()) | set(r_bm25.keys())\n",
    "\n",
    "    fused = []\n",
    "    for i in all_ids:\n",
    "        s = (1 / (k + r_dense.get(i, 9999))) + (1 / (k + r_bm25.get(i, 9999)))\n",
    "        fused.append((i, s))\n",
    "    fused = sorted(fused, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [{\"id\": i, \"score\": s, \"meta\": api_metadata[i]} for i, s in fused]\n",
    "\n",
    "\n",
    "print(\"\\nInitialization complete âœ…  You can now call:\")\n",
    "print('  print(response)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a38fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Which api responsible for check in flow\n",
      "\n",
      "Suggested API Workflow:\n",
      "\n",
      "Step 1: Creates a passive segment and creates a booking if it doesn't already exist.\n",
      "  Endpoint: /api/nsk/v1/trip/passiveSegments  |  Method: POST\n",
      "\n",
      "Step 2: Retrieves the booking payment methods available for the booking in state.\n",
      "  Endpoint: /api/nsk/v2/booking/payments/available  |  Method: GET\n",
      "  Parameters: currencyCode\n",
      "\n",
      "Step 3: Gets credit available by reference number and type.\n",
      "  Endpoint: /api/nsk/v2/booking/payments/credit  |  Method: GET\n",
      "  Parameters: ReferenceNumber, CurrencyCode, Type\n",
      "\n",
      "Step 4: Retrieves the booking payment methods available for a refund on the booking in state.\n",
      "  Endpoint: /api/nsk/v1/booking/payments/refunds  |  Method: GET\n",
      "\n",
      "Step 5: Gets the list of seat maps for all the journeys for the booking in state.\n",
      "  Endpoint: /api/nsk/v3/booking/seatmaps  |  Method: GET\n",
      "  Parameters: FeePricingMode, CollectedCurrencyCode, IncludePropertyLookup, CultureCode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Which api responsible for check in flow\"\n",
    "plan = generate_plan(query, top_candidates)\n",
    "print(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145662e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbbfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
