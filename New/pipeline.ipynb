{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48555443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading API metadata...\n",
      "Loaded 1024 API entries\n",
      "Loading FAISS index...\n",
      "FAISS index loaded. vectors: 1024\n",
      "Loading embedding model: C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\models\\all-MiniLM-L6-v2 ...\n",
      "Embedding model loaded.\n",
      "Building BM25 index...\n",
      "BM25 ready.\n",
      "Reranker not available at 'CrossEncoder(\n",
      "  (model): BertForSequenceClassification(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 384)\n",
      "        (token_type_embeddings): Embedding(2, 384)\n",
      "        (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSdpaSelfAttention(\n",
      "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Linear(in_features=384, out_features=1, bias=True)\n",
      "  )\n",
      "  (activation_fn): Identity()\n",
      ")'. Using RRF fallback.\n",
      "\n",
      "Initialization complete âœ…  You can now call:\n",
      "  print(response)\n"
     ]
    }
   ],
   "source": [
    "# ====== Initialization Cell (Run ONCE) ======\n",
    "# Loads embedding model, FAISS index, metadata, BM25, (optional) reranker.\n",
    "# After this cell, call: answer_query(\"your question\") as many times as you want.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -------- CONFIG (update paths if needed) ----------\n",
    "API_MAPPING_FILE = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\api_unified_data_full.json\"    # output from Step 2\n",
    "FAISS_INDEX_FILE = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\updated_api_index.faiss\"     # output from Step 2\n",
    "EMBED_MODEL_NAME = r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\models\\all-MiniLM-L6-v2\"\n",
    "\n",
    "# Optional local reranker (cross-encoder).\n",
    "# If this path is invalid or missing HF files, the code will safely fall back to RRF (non-LLM).\n",
    "RERANKER_MODEL = CrossEncoder(r\"C:\\Users\\Arpit.x.Tripathi\\Downloads\\Rag_chatbot\\Reranking-Model\\local-ms-marco\")\n",
    "# You can also use the hub id directly (requires internet):\n",
    "# RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "print(\"Loading API metadata...\")\n",
    "with open(API_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    api_metadata = json.load(f)\n",
    "print(f\"Loaded {len(api_metadata)} API entries\")\n",
    "\n",
    "print(\"Loading FAISS index...\")\n",
    "index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "print(\"FAISS index loaded. vectors:\", index.ntotal)\n",
    "\n",
    "print(f\"Loading embedding model: {EMBED_MODEL_NAME} ...\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# ---------- Text cleaning / tokenization ----------\n",
    "def clean_text_tokens(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', ' ', text).lower()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Add parameters & tags into corpus text to improve relevance\n",
    "def metadata_to_text(m):\n",
    "    params = m.get('parameters', []) or []\n",
    "    ptext = \" \".join([f\"{p.get('in','')}:{p.get('name','')}:{p.get('type','')}\" for p in params])\n",
    "    rtext = \" \".join([f\"{code}:{resp.get('description','')}\" for code, resp in (m.get('responses') or {}).items()])\n",
    "    return \" | \".join([\n",
    "        str(m.get('path','')),\n",
    "        str(m.get('method','')),\n",
    "        str(m.get('summary','')),\n",
    "        str(m.get('description','')),\n",
    "        \"tags:\" + \" \".join(m.get('tags',[])),\n",
    "        \"params:\" + ptext,\n",
    "        \"responses:\" + rtext\n",
    "    ])\n",
    "\n",
    "print(\"Building BM25 index...\")\n",
    "bm25_corpus = []\n",
    "for m in api_metadata:\n",
    "    txt = metadata_to_text(m)\n",
    "    bm25_corpus.append(clean_text_tokens(txt))\n",
    "bm25 = BM25Okapi(bm25_corpus)\n",
    "print(\"BM25 ready.\")\n",
    "\n",
    "# --------- (Optional) Reranker loading (safe) ----------\n",
    "reranker = None\n",
    "reranker_device = \"cpu\"\n",
    "\n",
    "def _has_hf_files(model_dir: str) -> bool:\n",
    "    \"\"\"Check if a local folder looks like a valid HF model (minimal check).\"\"\"\n",
    "    if not os.path.isdir(model_dir):\n",
    "        return False\n",
    "    needed = [\"config.json\"]  # minimal; you can add more like tokenizer_config.json, pytorch_model.bin\n",
    "    return all(os.path.isfile(os.path.join(model_dir, f)) for f in needed)\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    if isinstance(RERANKER_MODEL, str) and os.path.isdir(RERANKER_MODEL) and _has_hf_files(RERANKER_MODEL):\n",
    "        print(f\"Loading reranker from local path: {RERANKER_MODEL} ...\")\n",
    "        reranker = CrossEncoder(RERANKER_MODEL, device=reranker_device, local_files_only=True)\n",
    "        print(\"Reranker loaded (local).\")\n",
    "    elif isinstance(RERANKER_MODEL, str) and \"/\" in RERANKER_MODEL:\n",
    "        print(f\"Loading reranker from HF Hub: {RERANKER_MODEL} ...\")\n",
    "        reranker = CrossEncoder(RERANKER_MODEL, device=reranker_device)\n",
    "        print(\"Reranker loaded (hub).\")\n",
    "    else:\n",
    "        print(f\"Reranker not available at '{RERANKER_MODEL}'. Using RRF fallback.\")\n",
    "        reranker = None\n",
    "except Exception as e:\n",
    "    print(\"Failed to load reranker; continuing without it. Error:\", str(e))\n",
    "    reranker = None\n",
    "\n",
    "# --------- Utility functions ----------\n",
    "def normalize_query(q: str) -> str:\n",
    "    \"\"\"Lightweight typo fix & phrasing cleanup to improve retrieval without LLMs.\"\"\"\n",
    "    q_norm = (q or \"\").lower().strip()\n",
    "    fixes = {\n",
    "        \"dtails\": \"details\",\n",
    "        \"responsible for get\": \"get\",\n",
    "        \"api responsible for\": \"api for\",\n",
    "        \"booking dtail\": \"booking detail\",\n",
    "        \"bokking\": \"booking\",\n",
    "        \"resrvation\": \"reservation\",\n",
    "    }\n",
    "    for k, v in fixes.items():\n",
    "        q_norm = q_norm.replace(k, v)\n",
    "    q_norm = re.sub(r\"\\s+\", \" \", q_norm).strip()\n",
    "    return q_norm\n",
    "\n",
    "def embed_query(query: str):\n",
    "    vec = embed_model.encode([query], convert_to_numpy=True)\n",
    "    if vec.ndim == 1:\n",
    "        vec = vec.reshape(1, -1)\n",
    "    return vec.astype(\"float32\")\n",
    "\n",
    "def dense_search(query_vec, top_k=50):\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    results = []\n",
    "    for idx, dist in zip(I[0], D[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        # Convert L2 distance to a similarity-like score (bounded, higher=better)\n",
    "        sim = 1.0 / (1.0 + float(dist))\n",
    "        results.append((int(idx), sim))\n",
    "    return results\n",
    "\n",
    "def bm25_search(query, top_k=50):\n",
    "    tokens = clean_text_tokens(query)\n",
    "    scores = bm25.get_scores(tokens)\n",
    "    ranked = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(int(i), float(scores[i])) for i in ranked]\n",
    "\n",
    "def _minmax(arr):\n",
    "    \"\"\"Safe MinMax scaling for list/np array; returns zeros if degenerate.\"\"\"\n",
    "    arr = np.array(arr, dtype=float).reshape(-1,1)\n",
    "    if arr.size == 0:\n",
    "        return np.array([])\n",
    "    if np.allclose(arr.min(), arr.max()):\n",
    "        return np.zeros_like(arr).flatten()\n",
    "    return MinMaxScaler().fit_transform(arr).flatten()\n",
    "\n",
    "def hybrid_retrieve(query, top_k=20, bm25_weight=0.35):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval combining dense (FAISS) and BM25 with MinMax normalization.\n",
    "    Returns list of dicts: {'id', 'score', 'meta'}\n",
    "    \"\"\"\n",
    "    # Use normalized query for retrieval (keep original for display)\n",
    "    query_norm = normalize_query(query)\n",
    "\n",
    "    qvec = embed_query(query_norm)\n",
    "    dense = dense_search(qvec, top_k*2)\n",
    "    bm25_res = bm25_search(query_norm, top_k*2)\n",
    "\n",
    "    d_norm = _minmax([s for _, s in dense]) if len(dense) > 0 else np.array([])\n",
    "    b_norm = _minmax([s for _, s in bm25_res]) if len(bm25_res) > 0 else np.array([])\n",
    "\n",
    "    dense_dict = {dense[i][0]: float(d_norm[i]) for i in range(len(dense))} if len(dense) > 0 else {}\n",
    "    bm25_dict = {bm25_res[i][0]: float(b_norm[i]) for i in range(len(bm25_res))} if len(bm25_res) > 0 else {}\n",
    "\n",
    "    all_ids = set(list(dense_dict.keys()) + list(bm25_dict.keys()))\n",
    "    hybrid = []\n",
    "    for idx in all_ids:\n",
    "        d = dense_dict.get(idx, 0.0)\n",
    "        b = bm25_dict.get(idx, 0.0)\n",
    "        score = (1 - bm25_weight) * d + bm25_weight * b\n",
    "        hybrid.append((idx, float(score)))\n",
    "    hybrid = sorted(hybrid, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [{\"id\": idx, \"score\": s, \"meta\": api_metadata[idx]} for idx, s in hybrid]\n",
    "\n",
    "def rerank_candidates(query: str, candidates: list, top_k=5):\n",
    "    \"\"\"\n",
    "    Use cross-encoder to rerank candidate list if available.\n",
    "    Returns top_k list with 'rerank_score'.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "    if reranker is None:\n",
    "        ranked = sorted(candidates, key=lambda x: x[\"score\"], reverse=True)[:top_k]\n",
    "        for r in ranked:\n",
    "            r[\"rerank_score\"] = None\n",
    "        return ranked\n",
    "\n",
    "    pair_texts = []\n",
    "    for c in candidates:\n",
    "        m = c[\"meta\"]\n",
    "        text = metadata_to_text(m)\n",
    "        pair_texts.append((normalize_query(query), text))\n",
    "\n",
    "    scores = reranker.predict(pair_texts)\n",
    "    for i, s in enumerate(scores):\n",
    "        candidates[i][\"rerank_score\"] = float(s)\n",
    "    ranked = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)[:top_k]\n",
    "    return ranked\n",
    "\n",
    "# ---------- Parameter helpers & Output formatting ----------\n",
    "def _extract_params(m):\n",
    "    \"\"\"Normalized parameter records (optional; kept for future use).\"\"\"\n",
    "    params = m.get(\"parameters\", []) or []\n",
    "    out = []\n",
    "    for p in params:\n",
    "        out.append({\n",
    "            \"name\": p.get(\"name\", \"\"),\n",
    "            \"in\": p.get(\"in\", \"query\"),\n",
    "            \"type\": (p.get(\"type\") or (p.get(\"schema\", {}) or {}).get(\"type\") or \"string\"),\n",
    "            \"required\": bool(p.get(\"required\", False)),\n",
    "            \"description\": p.get(\"description\", \"\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def _param_names_only(m):\n",
    "    \"\"\"Return a comma-separated list of parameter NAMES (no type/in/required).\"\"\"\n",
    "    params = m.get(\"parameters\", []) or []\n",
    "    names = []\n",
    "    for p in params:\n",
    "        name = (p.get(\"name\") or \"\").strip()\n",
    "        if name and name != \"(body)\":\n",
    "            names.append(name)\n",
    "    # unique but keep order\n",
    "    seen = set()\n",
    "    uniq = [n for n in names if not (n in seen or seen.add(n))]\n",
    "    return \", \".join(uniq)\n",
    "\n",
    "def pretty_workflow_text(query: str, reranked: list) -> str:\n",
    "    \"\"\"\n",
    "    Prints exactly in the requested format:\n",
    "    User Query: ...\n",
    "    \n",
    "    Suggested API Workflow:\n",
    "    \n",
    "    Step i: <summary or path>.\n",
    "      Endpoint: <path>  |  Method: <METHOD>\n",
    "      Parameters: <CommaSeparatedParamNames>   # only if any\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    # Header\n",
    "    lines.append(f\"User Query: {query.strip()}\\n\")\n",
    "    lines.append(\"Suggested API Workflow:\\n\")\n",
    "\n",
    "    for i, c in enumerate(reranked, start=1):\n",
    "        m = c[\"meta\"]\n",
    "        summary = (m.get(\"summary\") or m.get(\"description\") or m.get(\"path\") or \"\").strip()\n",
    "        if summary and not summary.endswith(\".\"):\n",
    "            summary += \".\"\n",
    "        path = m.get(\"path\", \"\").strip()\n",
    "        method = (m.get(\"method\", \"GET\") or \"GET\").upper()\n",
    "        param_names = _param_names_only(m)\n",
    "\n",
    "        lines.append(f\"Step {i}: {summary}\")\n",
    "        lines.append(f\"  Endpoint: {path}  |  Method: {method}\")\n",
    "        if param_names:\n",
    "            lines.append(f\"  Parameters: {param_names}\")\n",
    "        lines.append(\"\")  # blank line after each step\n",
    "\n",
    "    # Remove trailing blank line\n",
    "    if lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- Reciprocal Rank Fusion (fallback if no reranker) ----------\n",
    "def rrf_fusion(dense_results, bm25_results, k=60, top_k=5):\n",
    "    \"\"\"\n",
    "    dense_results: list[(idx, score)] sorted desc by dense\n",
    "    bm25_results:  list[(idx, score)] sorted desc by bm25\n",
    "    Returns list of dicts like hybrid_retrieve but using RRF.\n",
    "    \"\"\"\n",
    "    def rank_map(res):\n",
    "        return {i: r for r, (i, _) in enumerate(res, start=1)}\n",
    "\n",
    "    d_sorted = sorted(dense_results, key=lambda x: x[1], reverse=True)\n",
    "    b_sorted = sorted(bm25_results, key=lambda x: x[1], reverse=True)\n",
    "    r_dense = rank_map(d_sorted)\n",
    "    r_bm25 = rank_map(b_sorted)\n",
    "\n",
    "    # Proper union of sets\n",
    "    all_ids = set(r_dense.keys()) | set(r_bm25.keys())\n",
    "\n",
    "    fused = []\n",
    "    for i in all_ids:\n",
    "        s = (1 / (k + r_dense.get(i, 9999))) + (1 / (k + r_bm25.get(i, 9999)))\n",
    "        fused.append((i, s))\n",
    "    fused = sorted(fused, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [{\"id\": i, \"score\": s, \"meta\": api_metadata[i]} for i, s in fused]\n",
    "\n",
    "\n",
    "print(\"\\nInitialization complete âœ…  You can now call:\")\n",
    "print('  print(response)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8398663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§© Final Developer Response:\n",
      "\n",
      "User Query: Which api responsible for check in flow\n",
      "\n",
      "Suggested API Workflow:\n",
      "\n",
      "Step 1: Gets the document check status for a specific passenger if there are any.\n",
      "  Endpoint: /api/nsk/v1/booking/passengers/{passengerKey}/documents/check  |  Method: GET\n",
      "  Parameters: passengerKey\n",
      "\n",
      "Step 2: Performs the health check.\n",
      "  Endpoint: /api/v3/health  |  Method: GET\n",
      "\n",
      "Step 3: Gets the passengers lift status for a specific journey based on the booking in state.\n",
      "  Endpoint: /api/nsk/v1/booking/checkin/journey/{journeyKey}/status  |  Method: GET\n",
      "  Parameters: journeyKey\n",
      "\n",
      "Step 4: Gets the checkin pre-validation requirements for a specific journey.\n",
      "  Endpoint: /api/nsk/v2/booking/checkin/journey/{journeyKey}/requirements  |  Method: GET\n",
      "  Parameters: journeyKey\n",
      "\n",
      "Step 5: Gets the document check status for a specific passenger's infant if there are any.\n",
      "  Endpoint: /api/nsk/v1/booking/passengers/{passengerKey}/infant/documents/check  |  Method: GET\n",
      "  Parameters: passengerKey\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = answer_query(\"Which api responsible for check in flow\", use_reranker=False)\n",
    "print(\"\\nðŸ§© Final Developer Response:\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c844a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
